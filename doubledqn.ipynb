{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-14T12:14:06.417424Z","iopub.execute_input":"2024-05-14T12:14:06.417760Z","iopub.status.idle":"2024-05-14T12:14:07.306494Z","shell.execute_reply.started":"2024-05-14T12:14:06.417732Z","shell.execute_reply":"2024-05-14T12:14:07.305533Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import gym\nfrom gym import spaces\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom collections import deque\nimport random\nfrom itertools import count\nimport torch.nn.functional as F\nfrom tensorboardX import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2024-05-14T12:14:07.308361Z","iopub.execute_input":"2024-05-14T12:14:07.308799Z","iopub.status.idle":"2024-05-14T12:14:17.245154Z","shell.execute_reply.started":"2024-05-14T12:14:07.308774Z","shell.execute_reply":"2024-05-14T12:14:17.244089Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device.type)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T12:14:17.246375Z","iopub.execute_input":"2024-05-14T12:14:17.246904Z","iopub.status.idle":"2024-05-14T12:14:17.282234Z","shell.execute_reply.started":"2024-05-14T12:14:17.246878Z","shell.execute_reply":"2024-05-14T12:14:17.281002Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"class ValueNetwork(nn.Module):\n    def __init__(self):\n        super(ValueNetwork, self).__init__()\n\n        self.fc1 = nn.Linear(512, 1024)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 512)\n\n    def forward(self, dialog_current_state):\n        y = self.relu(self.fc1(dialog_current_state))\n        y = self.relu(self.fc2(y))\n        current_state = self.fc3(y)\n\n        return current_state\n\n    def current_state(self, dialog_current_state):\n        with torch.no_grad():\n            current_state = self.forward(dialog_current_state)\n            \n        return current_state\n    \nvalueNetwork = ValueNetwork().to(device)\ndialog_current_state = torch.randn((512,), dtype=torch.float32, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T12:14:17.284631Z","iopub.execute_input":"2024-05-14T12:14:17.285337Z","iopub.status.idle":"2024-05-14T12:14:17.496348Z","shell.execute_reply.started":"2024-05-14T12:14:17.285297Z","shell.execute_reply":"2024-05-14T12:14:17.495508Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nclass DialogEnv(gym.Env):\n    def __init__(self):\n        super(DialogEnv, self).__init__()\n        # Định nghĩa không gian hành động và không gian trạng thái\n        self.action_space = spaces.Discrete(2)  # Hành động là 0 hoặc 1\n        self.observation_space = spaces.Box(low=-float('inf'), high=float('inf'), shape=())  # Không gian trạng thái là không biết trước\n\n        # Khởi tạo vector trạng thái ban đầu\n        self.state = valueNetwork.current_state(dialog_current_state).to(device)\n\n        self.current_state = None\n        self.max_steps = 20\n        self.current_step = 0\n\n    def step(self, action):\n        self.current_step += 1\n        done = False\n\n        # Xác định phần thưởng dựa trên hành động và trạng thái hiện tại\n        if action == 0: # recommend\n            if np.random.random() < 0.8:  # Khả năng thành công\n                reward = 1  # rec_suc: cộng nhiều điểm\n            else:\n                reward = -1  # rec_fail: trừ nhiều điểm\n        else:  # ask\n            if np.random.random() < 0.6:  # Khả năng thành công\n                reward = 0.3  # ask_suc: cộng ít điểm\n            else:\n                reward = -0.1  # ask_fail: trừ ít điểm\n\n        # Diễn ra tình hình trạng thái tiếp theo\n        if self.current_step >= self.max_steps:\n            reward -= 0.7\n            done = True\n        else:\n            self.current_state = valueNetwork.current_state(dialog_current_state).to(device)\n\n        return self.current_state, reward, done, {}\n\n    def reset(self):\n        # Khởi tạo trạng thái ban đầu\n        self.current_step = 0\n        self.state = valueNetwork.current_state(dialog_current_state).to(device)\n        return self.state\n\n\n# Đăng ký môi trường\ngym.register(\"DialogEnv-v0\", entry_point=DialogEnv)\n\n# Sử dụng môi trường\nenv = gym.make(\"DialogEnv-v0\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T12:14:17.497616Z","iopub.execute_input":"2024-05-14T12:14:17.497918Z","iopub.status.idle":"2024-05-14T12:14:17.635490Z","shell.execute_reply.started":"2024-05-14T12:14:17.497894Z","shell.execute_reply":"2024-05-14T12:14:17.634574Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: ()\u001b[0m\n  logger.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# env.reset()\n# for _ in range(21):\n    \n#     action = env.action_space.sample()  # Lấy một hành động ngẫu nhiên\n\n#     state, reward, done, info = env.step(action)\n\n#     print(\"State:\", state, \"Action:\", action, \"Reward:\", reward, 'Info:', info, \"Current_step:\", env.current_step)\n#     if done:\n#         print(\"Episode finished after\", _ + 1, \"timesteps\")\n#         break\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-14T12:14:17.636602Z","iopub.execute_input":"2024-05-14T12:14:17.636859Z","iopub.status.idle":"2024-05-14T12:14:17.641339Z","shell.execute_reply.started":"2024-05-14T12:14:17.636837Z","shell.execute_reply":"2024-05-14T12:14:17.640290Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\nclass DQN(nn.Module):\n    def __init__(self):\n        super(DQN, self).__init__()\n\n        self.fc1 = nn.Linear(512, 1024)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 2)\n\n    def forward(self, state):\n        y = self.relu(self.fc1(state))\n        y = self.relu(self.fc2(y))\n        Q = self.fc3(y)\n\n        return Q\n\n    def select_action(self, state):\n        with torch.no_grad():\n            Q = self.forward(state)\n            action_index = torch.argmax(Q, dim=1)\n        return action_index.item()\n\n\nclass Memory(object):\n    def __init__(self, memory_size: int) -> None:\n        self.memory_size = memory_size\n        self.buffer = deque(maxlen=self.memory_size)\n\n    def add(self, experience) -> None:\n        self.buffer.append(experience)\n\n    def size(self):\n        return len(self.buffer)\n\n    def sample(self, batch_size: int, continuous: bool = True):\n        if batch_size > len(self.buffer):\n            batch_size = len(self.buffer)\n        if continuous:\n            rand = random.randint(0, len(self.buffer) - batch_size)\n            return [self.buffer[i] for i in range(rand, rand + batch_size)]\n        else:\n            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n            return [self.buffer[i] for i in indexes]\n\n    def clear(self):\n        self.buffer.clear()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T12:14:17.642652Z","iopub.execute_input":"2024-05-14T12:14:17.643062Z","iopub.status.idle":"2024-05-14T12:14:17.657876Z","shell.execute_reply.started":"2024-05-14T12:14:17.643031Z","shell.execute_reply":"2024-05-14T12:14:17.657040Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\nonlineQNetwork = DQN().to(device)\ntargetQNetwork = DQN().to(device)\ntargetQNetwork.load_state_dict(onlineQNetwork.state_dict())\n\noptimizer = torch.optim.Adam(onlineQNetwork.parameters(), lr=1e-4)\n\nGAMMA = 0.99\nEPSILON_DECAY = 0.999\nINITIAL_EPSILON = 1\nFINAL_EPSILON = 0.001\nREPLAY_MEMORY = 1000\nBATCH = 16\nUPDATE_STEPS = 4\n\nmemory_replay = Memory(REPLAY_MEMORY)\nepsilon = INITIAL_EPSILON\nlearn_steps = 0\nwriter = SummaryWriter('logs/dqn')\nbegin_learn = False\nepochs = 1000\n\n# for epoch in count():\nfor epoch in range(epochs):\n    state = env.reset()\n    \n    episode_reward = 0\n    for time_steps in range(200):\n        p = random.random()\n        if p < epsilon:\n            action = random.randint(0, 1)\n        else:\n            tensor_state = state.unsqueeze(0).to(device)\n#             print(tensor_state)\n            action = onlineQNetwork.select_action(tensor_state)\n        next_state, reward, done, _ = env.step(action)\n        episode_reward += reward\n        memory_replay.add((state, next_state, action, reward, done))\n        if memory_replay.size() >= REPLAY_MEMORY:\n            if begin_learn is False:\n                print('learn begin!')\n                begin_learn = True\n            learn_steps += 1\n            if learn_steps % UPDATE_STEPS == 0:\n                targetQNetwork.load_state_dict(onlineQNetwork.state_dict())\n            batch = memory_replay.sample(BATCH, True)\n            batch_state, batch_next_state, batch_action, batch_reward, batch_done = zip(*batch)\n\n            batch_state = torch.stack(batch_state).to(device)\n            batch_next_state = torch.stack(batch_next_state).to(device)\n            batch_action = torch.FloatTensor(batch_action).unsqueeze(1).to(device)\n            batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(device)\n            batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(device)\n\n            with torch.no_grad():\n                targetQ_next = targetQNetwork(batch_next_state)\n                y = batch_reward + (1 - batch_done) * GAMMA * torch.max(targetQ_next, dim=1, keepdim=True)[0]\n\n            loss = F.mse_loss(onlineQNetwork(batch_state).gather(1, batch_action.long()), y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            writer.add_scalar('loss', loss.item(), global_step=learn_steps)\n\n            if epsilon > FINAL_EPSILON:\n                epsilon *= EPSILON_DECAY\n\n        if done:\n            break\n        state = next_state\n\n    writer.add_scalar('episode reward', episode_reward, global_step=epoch)\n    if epoch % 10 == 0:\n        torch.save(onlineQNetwork.state_dict(), 'dqn-policy.para')\n        print('Ep {}\\tMoving average score: {:.2f}\\t'.format(epoch, episode_reward))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-14T12:36:17.817502Z","iopub.execute_input":"2024-05-14T12:36:17.818231Z","iopub.status.idle":"2024-05-14T12:37:13.624065Z","shell.execute_reply.started":"2024-05-14T12:36:17.818195Z","shell.execute_reply":"2024-05-14T12:37:13.623298Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Ep 0\tMoving average score: 7.40\t\nEp 10\tMoving average score: 5.80\t\nEp 20\tMoving average score: 1.60\t\nEp 30\tMoving average score: 10.30\t\nEp 40\tMoving average score: 8.50\t\nlearn begin!\nEp 50\tMoving average score: 3.90\t\nEp 60\tMoving average score: 8.50\t\nEp 70\tMoving average score: 8.90\t\nEp 80\tMoving average score: 7.40\t\nEp 90\tMoving average score: 10.40\t\nEp 100\tMoving average score: 2.60\t\nEp 110\tMoving average score: 10.30\t\nEp 120\tMoving average score: 13.30\t\nEp 130\tMoving average score: 10.60\t\nEp 140\tMoving average score: 13.50\t\nEp 150\tMoving average score: 10.60\t\nEp 160\tMoving average score: 1.30\t\nEp 170\tMoving average score: 12.00\t\nEp 180\tMoving average score: 13.30\t\nEp 190\tMoving average score: 14.20\t\nEp 200\tMoving average score: 17.30\t\nEp 210\tMoving average score: 9.50\t\nEp 220\tMoving average score: 10.60\t\nEp 230\tMoving average score: 2.60\t\nEp 240\tMoving average score: 16.20\t\nEp 250\tMoving average score: 7.30\t\nEp 260\tMoving average score: 16.20\t\nEp 270\tMoving average score: 11.30\t\nEp 280\tMoving average score: 17.30\t\nEp 290\tMoving average score: 7.30\t\nEp 300\tMoving average score: 3.40\t\nEp 310\tMoving average score: 13.30\t\nEp 320\tMoving average score: 13.30\t\nEp 330\tMoving average score: 11.30\t\nEp 340\tMoving average score: 9.30\t\nEp 350\tMoving average score: 9.30\t\nEp 360\tMoving average score: 9.30\t\nEp 370\tMoving average score: 15.30\t\nEp 380\tMoving average score: 11.30\t\nEp 390\tMoving average score: 13.30\t\nEp 400\tMoving average score: 9.30\t\nEp 410\tMoving average score: 15.30\t\nEp 420\tMoving average score: 13.30\t\nEp 430\tMoving average score: 9.30\t\nEp 440\tMoving average score: 15.30\t\nEp 450\tMoving average score: 13.30\t\nEp 460\tMoving average score: 7.30\t\nEp 470\tMoving average score: 13.30\t\nEp 480\tMoving average score: 13.30\t\nEp 490\tMoving average score: 17.30\t\nEp 500\tMoving average score: 13.30\t\nEp 510\tMoving average score: 7.30\t\nEp 520\tMoving average score: 13.30\t\nEp 530\tMoving average score: 15.30\t\nEp 540\tMoving average score: 9.30\t\nEp 550\tMoving average score: 5.30\t\nEp 560\tMoving average score: 11.30\t\nEp 570\tMoving average score: 11.30\t\nEp 580\tMoving average score: 15.30\t\nEp 590\tMoving average score: 11.30\t\nEp 600\tMoving average score: 11.30\t\nEp 610\tMoving average score: 13.30\t\nEp 620\tMoving average score: 13.30\t\nEp 630\tMoving average score: 7.30\t\nEp 640\tMoving average score: 11.30\t\nEp 650\tMoving average score: 7.30\t\nEp 660\tMoving average score: 7.30\t\nEp 670\tMoving average score: 13.30\t\nEp 680\tMoving average score: 13.30\t\nEp 690\tMoving average score: 15.30\t\nEp 700\tMoving average score: 15.30\t\nEp 710\tMoving average score: 13.30\t\nEp 720\tMoving average score: 13.30\t\nEp 730\tMoving average score: 5.30\t\nEp 740\tMoving average score: 11.30\t\nEp 750\tMoving average score: 15.30\t\nEp 760\tMoving average score: 9.30\t\nEp 770\tMoving average score: 7.30\t\nEp 780\tMoving average score: 11.30\t\nEp 790\tMoving average score: 11.30\t\nEp 800\tMoving average score: 15.30\t\nEp 810\tMoving average score: 7.30\t\nEp 820\tMoving average score: 11.30\t\nEp 830\tMoving average score: 11.30\t\nEp 840\tMoving average score: 4.70\t\nEp 850\tMoving average score: 2.10\t\nEp 860\tMoving average score: 15.30\t\nEp 870\tMoving average score: 11.30\t\nEp 880\tMoving average score: 11.30\t\nEp 890\tMoving average score: 12.20\t\nEp 900\tMoving average score: 11.30\t\nEp 910\tMoving average score: 9.30\t\nEp 920\tMoving average score: 7.30\t\nEp 930\tMoving average score: 9.30\t\nEp 940\tMoving average score: 13.30\t\nEp 950\tMoving average score: 11.30\t\nEp 960\tMoving average score: 11.30\t\nEp 970\tMoving average score: 11.30\t\nEp 980\tMoving average score: 11.30\t\nEp 990\tMoving average score: 17.30\t\n","output_type":"stream"}]}]}